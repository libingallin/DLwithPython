{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T13:14:40.887854Z",
     "start_time": "2018-08-28T13:14:40.884384Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T12:55:07.192359Z",
     "start_time": "2018-08-28T12:55:06.848081Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T12:55:07.907683Z",
     "start_time": "2018-08-28T12:55:07.898259Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T12:55:09.044721Z",
     "start_time": "2018-08-28T12:55:08.767475Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据预处理，将其变换为网络要求的形状，并缩放到[0, 1]之间\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T12:55:09.385803Z",
     "start_time": "2018-08-28T12:55:09.337692Z"
    }
   },
   "outputs": [],
   "source": [
    "# 网络架构\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation=\"relu\", input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T12:55:25.274449Z",
     "start_time": "2018-08-28T12:55:25.238737Z"
    }
   },
   "outputs": [],
   "source": [
    "# 编译，compile\n",
    "network.compile(optimizer=\"rmsprop\",\n",
    "                loss=\"categorical_crossentropy\",\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T12:55:25.979571Z",
     "start_time": "2018-08-28T12:55:25.973088Z"
    }
   },
   "outputs": [],
   "source": [
    "# 准备标签\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T12:55:39.842657Z",
     "start_time": "2018-08-28T12:55:26.843125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.2552 - acc: 0.9256\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1041 - acc: 0.9688\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.0690 - acc: 0.9794\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.0510 - acc: 0.9842\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.0381 - acc: 0.9889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1750b14e9e8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit\n",
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> loss: 网络在训练集上的损失  \n",
    "acc: 网络在训练数据集上的accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T12:55:44.632445Z",
     "start_time": "2018-08-28T12:55:44.163261Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 46us/step\n",
      "Accuracy on test images: 0.9803\n"
     ]
    }
   ],
   "source": [
    "# 在测试集上评估性能\n",
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print(\"Accuracy on test images: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 训练集accuracy为98.03%，比训练集accuracy（98.89%）低不少，**过拟合**(指在新数据上的性能往往比在训练集上要差)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 电影评论分类：二分类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ IMDB数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50000条严重两极化的评论，一半用于训练，一半用于测试，分别都包含一半正面评论，一半负面评论。\n",
    "\n",
    "已经经过预处理：评论(单词序列）已经被转化为整数序列，其中每个整数代表字典中的某个单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T13:18:54.418350Z",
     "start_time": "2018-08-28T13:18:50.066874Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "# num_words=10000指仅保留训练数据中前10000个最长出现的单词,低频单词将被舍弃\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
    "    num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `train_data`, `test_data`这两个变量是评论组成的列表，每条评论又是单词索引组成的列表（表示一系列单词）。  \n",
    "`train_labels`, `test_labels`都是0和1组成的列表，0代表负面，1代表正面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T13:02:46.998460Z",
     "start_time": "2018-08-28T13:02:46.993499Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T13:05:46.147332Z",
     "start_time": "2018-08-28T13:05:46.142372Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
       "       list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 8255, 2, 349, 2637, 148, 605, 2, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
       "       list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T13:04:55.539478Z",
     "start_time": "2018-08-28T13:04:55.412261Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 单词索引不会超过10000，因为限定为前10000个最常见的单词\n",
    "max(max(sequence) for sequence in train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T13:21:37.894404Z",
     "start_time": "2018-08-28T13:21:37.888451Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**将列表转换为张量**。方法有二：\n",
    "1. 填充列表，使其具有相同的长度，再将列表转换成形状为(samples, word_indices)的整数张量，然后网络第一层可以使用能处理这种整数张量的层（即`Embedding`层）；\n",
    "2. 对列表进行one-hot编码，将其转换为0和1组成的向量。然后网络第一层可以使用`Dense`层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T13:19:42.510962Z",
     "start_time": "2018-08-28T13:19:42.506984Z"
    }
   },
   "outputs": [],
   "source": [
    "# 将整数序列编码为二进制矩阵\n",
    "def vectorize_sequences(sequences, dimensions=10000):\n",
    "    # 创建结果矩阵，shape=(len(sequences), dimensions)\n",
    "    results = np.zeros((len(sequences), dimensions))\n",
    "    # 该位置单词出现，则为1\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T13:19:48.219726Z",
     "start_time": "2018-08-28T13:19:43.946190Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T13:19:49.997161Z",
     "start_time": "2018-08-28T13:19:49.979302Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 10000)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T13:19:50.783166Z",
     "start_time": "2018-08-28T13:19:50.756877Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T13:20:52.804367Z",
     "start_time": "2018-08-28T13:20:52.798899Z"
    }
   },
   "outputs": [],
   "source": [
    "# 将标签向量化\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T13:21:47.303018Z",
     "start_time": "2018-08-28T13:21:47.294587Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., ..., 0., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 构建网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T13:40:07.404068Z",
     "start_time": "2018-08-28T13:40:07.361879Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000, )))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 编译模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T13:42:53.090746Z",
     "start_time": "2018-08-28T13:42:53.040644Z"
    }
   },
   "outputs": [],
   "source": [
    "# 编译模型\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以自定义这三个参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T13:45:08.184456Z",
     "start_time": "2018-08-28T13:45:08.146762Z"
    }
   },
   "outputs": [],
   "source": [
    "# 配置optimizer，传入一个optimizer实例\n",
    "from keras import optimizers\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T13:47:20.769189Z",
     "start_time": "2018-08-28T13:47:20.719108Z"
    }
   },
   "outputs": [],
   "source": [
    "# 使用自定义loss和metrics，传入函数对象\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss=losses.binary_crossentropy,\n",
    "              metrics=[metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将原始训练集保留出10000个样本作为验证集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T13:50:36.309520Z",
     "start_time": "2018-08-28T13:50:36.304557Z"
    }
   },
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-28T14:01:26.383048Z",
     "start_time": "2018-08-28T14:00:34.047606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 3s 190us/step - loss: 0.0114 - acc: 0.9969 - val_loss: 0.6976 - val_acc: 0.8634\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 3s 167us/step - loss: 0.0025 - acc: 0.9998 - val_loss: 0.7287 - val_acc: 0.8654\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 3s 172us/step - loss: 0.0040 - acc: 0.9995 - val_loss: 0.7572 - val_acc: 0.8667\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 3s 167us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.7915 - val_acc: 0.8655\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 3s 167us/step - loss: 0.0040 - acc: 0.9991 - val_loss: 0.8170 - val_acc: 0.8632\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 2s 166us/step - loss: 7.0375e-04 - acc: 1.0000 - val_loss: 0.8375 - val_acc: 0.8632\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 2s 165us/step - loss: 5.6376e-04 - acc: 1.0000 - val_loss: 0.8993 - val_acc: 0.8631\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 2s 165us/step - loss: 0.0024 - acc: 0.9995 - val_loss: 0.9143 - val_acc: 0.8616\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 3s 170us/step - loss: 3.0020e-04 - acc: 1.0000 - val_loss: 0.9266 - val_acc: 0.8621\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 3s 185us/step - loss: 2.3339e-04 - acc: 1.0000 - val_loss: 0.9503 - val_acc: 0.8602\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 3s 171us/step - loss: 0.0015 - acc: 0.9996 - val_loss: 0.9830 - val_acc: 0.8605\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 3s 169us/step - loss: 1.4235e-04 - acc: 1.0000 - val_loss: 0.9943 - val_acc: 0.8615\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 3s 168us/step - loss: 1.1518e-04 - acc: 1.0000 - val_loss: 1.0121 - val_acc: 0.8602\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 3s 172us/step - loss: 8.9311e-05 - acc: 1.0000 - val_loss: 1.1259 - val_acc: 0.8555\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 3s 176us/step - loss: 0.0015 - acc: 0.9997 - val_loss: 1.0500 - val_acc: 0.8592\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 3s 182us/step - loss: 5.8583e-05 - acc: 1.0000 - val_loss: 1.0635 - val_acc: 0.8595\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 3s 192us/step - loss: 4.9169e-05 - acc: 1.0000 - val_loss: 1.0863 - val_acc: 0.8585\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 3s 176us/step - loss: 0.0026 - acc: 0.9993 - val_loss: 1.1297 - val_acc: 0.8581\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 3s 171us/step - loss: 3.9369e-05 - acc: 1.0000 - val_loss: 1.1252 - val_acc: 0.8597\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 3s 180us/step - loss: 2.7754e-05 - acc: 1.0000 - val_loss: 1.1330 - val_acc: 0.8605\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# batch_size=512, epochs=20\n",
    "# validation_data监控在验证集上的loss和accuracy\n",
    "history = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
